# -*- coding: utf-8 -*-
"""Practice_code_00.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c8wIXvRP0zat6Q7_O8jURLZESUp7NeXg

###IMPORT FILE
"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print(torch.__version__)

"""###Introduction"""

##Introduction to Tensors
###Creating Tensors

"""###Scalar"""

scalar = torch.tensor(7)
scalar
#Scalar is 0 dimension

"""###Vector"""

vector = torch.tensor([7,7])
vector
#Vector is 1 dimension

"""###MATRIX"""

MATRIX = torch.tensor([[7,8],[9,10]])
MATRIX[0]

"""###TENSOR"""

#Tensor
TENSOR = torch.tensor([[[2,3,4],[3,4,5],[4,5,6]]])
TENSOR.shape

"""###RANDOM TENSOR

Random Tensor are important because they represent the first input given by the machine learning / deep learning model and then adjust the random tensor numbers to represent a better data.

Start with random number -> look at data -> update random numbers -> update random numbers.
"""

#Create a random tensor of size(3,4)
random_tensor = torch.rand(3,4)
random_tensor

#Create a random tensor with a similar shape to an image tensor
random_image_tensor = torch.rand(size = (3,224,224)) #height ,width ,color (R,G,B)
#size represent dimension (a,b,c) mean 3 dimensions 
random_image_tensor.shape

"""###ZEROES AND ONES"""

zero_tensor = torch.zeros(3,4)
zero_tensor

one = torch.ones(3,4)
one

"""###Create a range of tensors and tensors like"""

#Use torch.arange()
one_to_ten = torch.arange(1,11)
one_to_ten

step = torch.arange(start = 0, end = 1000, step = 123)
step

#Create a tensors like
ten_zero = torch.zeros_like(input=one_to_ten)
ten_zero

"""##Tensor Datatype

**Note:** 3 Errors you will get with datatype in pytorch and deeplearning:

1. Tensors not right datatype - to get dataype from tensor, use `tensor.dtype`
2. Tensors not right shape - to get a shape from tensor, use `tensor.shape`
3. Tensors not on right device - to get a device from tensor, use `tensor.device`

"""

#Float 32-tensor
#datatype of tensor(float32, float16, etc)
float_32_tensor = torch.tensor([3.0,6.0,9.0],
                               dtype=None,
                               device=None, ##tensor device of the GPU, TPU(cuda, etc)
                               requires_grad=False) #Wether or not track gradient with this tensors operation
float_32_tensor

float_32_tensor.dtype

float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor.dtype

int_32_tensor = torch.tensor([3,6,9], dtype = torch.int32)
int_32_tensor

int_32_tensor.dtype

float_16_tensor * int_32_tensor

"""##Getting Information from Tensor(Tensor Attribute)
1. Tensors not right datatype - to get dataype from tensor, use tensor.dtype
2. Tensors not right shape - to get a shape from tensor, use tensor.shape
3. Tensors not on right device - to get a device from tensor, use tensor.device
"""

#Create tensor
some_tensor = torch.rand(3,4)
some_tensor

#Find detail of a tensor
print(some_tensor)
print(f"Datatype of tensors : {some_tensor.dtype}")
print(f"Shape of tensor : {some_tensor.size()}")
print(f"Device of tensor : {some_tensor.device}")

"""###Manipulating Tensors(Tensor Operations)

Tensor Operation include:
* Addition
* Substraction
* Multiplication (element-wise)
* Division
* Matrix Multiplication
"""

#Create a Tensor and add 10 to it
tensor = torch.tensor([1,2,3])
torch.add(tensor,10) #same function
tensor + 10

#Multiply a tensor
tensor * 10 
torch.mul(tensor,10) #same function

#Substract tensor
tensor - 10

"""##Matrix Multiplication

Two ways of performing matrix multiplication in neural networks and deep learning:

1. Element-wise multiplication
2. Matrix multiplication (dot product {a . b})

Two main rule of performing matrix multiplication:
1. The **Inner dimension** must match:
- (3,2) (2,3) will work.
- (2,3) (2,3) won't work.
2. The resulting matrix has the shape of **outer dimension**"
- (3,2) (2,3) -> result shape : (3,3)
- (2,3) (3,2) -> result shape : (2,2)

"""

#Element-wise Multiplication
print(tensor, "*", tensor)
print(f"Equals: {tensor * tensor}")
#can use operation like tensor @ tensor

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #Matrix Multiplication
# torch.matmul(tensor,tensor) ##this is the fastest to calculate matrix multiplication
# #torch.mm is the same as torch.matmul
# torch.matmul(tensor,tensor)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(torch.rand(3,2),torch.rand(2,3))

"""To fix tensor shapes, we can manipulate using transpose."""

tensor_a = torch.rand(3,2)
tensor_b = torch.rand(3,2)
tensor_b = tensor_b.T #For transpose
torch.mm(tensor_a,tensor_b) ##from (3,2) @ (3,2) -> (3,2) @ (2,3)

"""##Finding the min, max, mean, sum, etc (aggregation)"""

#Create a tensor

x = torch.arange(0,101,10)
x, x.dtype

#find the min
torch.min(x), x.min()

#find the max
torch.max(x), x.max()

#Find the mean
torch.mean(x.type(torch.float32)), x.type(torch.float32).mean() #MEAN change type from int to floating point / long (float32)

#Find the sum
torch.sum(x), x.sum()

"""##Find the positional min and max"""

x.argmin(), x.argmax() ##index min value & index max value

"""##RESHAPING, STACKING, SQUEEZING, AND UNSQUEEZING TENSORS

* Reshape - reshapes an input tensor to a defined shape
* View - Return a view of an input tensor of certain shape but keep the same memory as the original tensor
* Stacking - Combine multiple tensors on top of each other (vstack) or side by side(hstack)
* Squeeze - remove all `1` dimensions from a tensor
* Unsqueeze - add a `1` dimension to a target tensor
* Permute - Return a view of the input dimensions permuted (swapped) in a certain dimension way
"""

#example
x = torch.arange(1,11,1)
x, x.shape

#Add an extra dimension

x_reshaped = x.reshape(1,10) #reshape must always have equal to original size
x_reshaped, x_reshaped.shape

#Change the view
z = x.view(1,10) #Z and X share same memory, changing Z means changing X as well.
z, z.shape

#Change Z if it change X as well
z[:,0] = 5
z,x

#Stack tensor of each other
x_stacked = torch.stack([x,x,x,x],dim=0)
x_stacked

#torch_squeeze
print(f"Previous tensor: {x_reshaped}")
print(f"Previous shape : {x_reshaped.shape}")

x_reshaped = x_reshaped.squeeze()

print(f"Tensor now : {x_reshaped}")
print(f"Shape now : {x_reshaped.shape}")

#torch_unSqueeze

print(f"Previous tensor: {x_reshaped}")
print(f"Previous shape : {x_reshaped.shape}")

x_reshaped = x_reshaped.unsqueeze(dim=0)

print(f"Tensor now : {x_reshaped}")
print(f"Shape now : {x_reshaped.shape}")

#torch_permute

x_original = torch.rand(size=(224,224,3)) #image data height, width, colour channel

#permute the original tensor to rearrange axis of dimension / order of the dimension
x_permuted = x_original.permute(2,0,1) #shift index 2->0, 0->1, 1->2
x_permuted.shape

"""##Indexing (Selecting Data from Tensors)"""

#Create Tensor
x = torch.arange(1,10).reshape(1,3,3)
x, x.shape

#Index on new tensor all dimension

x[0]

#Index on new tensor (dim=1)
x[0][0]

#Index on new tensor last dimension
x[0][0][0]

#use ":" to select "all" dimension
x[:, :, 1] #grab all dimension 0 and 1, and only grab dimension 2 of index 1

"""##Pytorch tensors  & NumPy

Numpy is a popular scientific python numerical computer library.
And because of this, Pytorch has functionality to interact with it.

* Data in NumPy, want in Pytorch tensor -> `torch.from_numpy(ndarray)`
* Pytorch tensor -> NumPy -> `torch.Tensor.numPy()`
"""

#NumPy array to tensor
import torch
import numpy as np

array = np.arange(1.0,8.0)
tensor = torch.from_numpy(array) ##warning, when converting from numpy to pytorch, pytorch reflect numpy's default dataType of float64 unless specified otherwise. Must convert it to float32 for torch
array,tensor

tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor

tensor.dtype



"""##Reproducibility (Trying to take random of random)

How Neural Network works:

`start with random numbers -> tensor operations -> update random numbers to try and make them of the data -> again -> again -> ...`

To reduce randomness in neural networks and pyTorch comes the concept of **random seed**.
Essentially what the random seed does in "flavour" the randomness.
"""

random_tensor_a = torch.rand(3,4)
random_tensor_b = torch.rand(3,4)
print(random_tensor_a)
print(random_tensor_b)
print(random_tensor_a == random_tensor_b)

#Set random seed
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
random_tensor_c = torch.rand(3,4)
torch.manual_seed(RANDOM_SEED)
random_tensor_d = torch.rand(3,4)
print(random_tensor_c)
print(random_tensor_d)
print(random_tensor_c == random_tensor_d)

"""##Running Tensors and pytorch objects on the GPUs (and making faster computations)

"""

